{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9cf87b",
   "metadata": {},
   "source": [
    "### Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245fc206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting pgvector\n",
      "  Downloading pgvector-0.4.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting httpx>=0.27 (from ollama)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic>=2.9 (from ollama)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy (from pgvector)\n",
      "  Downloading numpy-2.3.5-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio (from httpx>=0.27->ollama)\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting certifi (from httpx>=0.27->ollama)\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.27->ollama)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx>=0.27->ollama)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.27->ollama)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->ollama)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.9->ollama)\n",
      "  Downloading pydantic_core-2.41.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in ./venv/lib/python3.11/site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.9->ollama)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Downloading psycopg2_binary-2.9.11-cp311-cp311-macosx_11_0_arm64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pgvector-0.4.1-py3-none-any.whl (27 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m463.6/463.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.41.5-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.5-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-inspection, pydantic-core, psycopg2-binary, numpy, idna, h11, certifi, annotated-types, pydantic, pgvector, httpcore, anyio, httpx, ollama\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.12.0 certifi-2025.11.12 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 numpy-2.3.5 ollama-0.6.1 pgvector-0.4.1 psycopg2-binary-2.9.11 pydantic-2.12.5 pydantic-core-2.41.5 typing-inspection-0.4.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "89a2e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import fitz  \n",
    "import ollama \n",
    "import psycopg2 \n",
    "import numpy as np\n",
    "from pgvector.psycopg2 import register_vector\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cd1e7586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are now chating with llama3.2:3b (say 'exit' to quit)\n",
      "\n",
      "You:  helo, what is your name\n",
      "llama3.2:3b: Hello! I don't have a personal name, but I'm an AI designed to assist and communicate with users. You can call me \"Assistant\" or \"AI\" if you prefer. How can I help you today?\n",
      "\n",
      "You:  okay, so your name now is bob, ok?\n",
      "llama3.2:3b: Hi there! Nice to meet you. So, I'm Bob from now on. How can I assist you today?\n",
      "\n",
      "You:  what is your name?\n",
      "llama3.2:3b: I don't have a personal name, but I'm an AI designed to assist and communicate with users. You can think of me as a conversational companion or a helpful tool, rather than a person with a specific name. Some people refer to me as \"Assistant\" or \"AI\" for short. What's your name?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Base llm call\n",
    "ollama pull mistral:7b or ollama pull qwen2.5:7b\n",
    "\"\"\"\n",
    "MODEL = \"llama3.2:3b\"  \n",
    "# mistral:7b, qwen2.5:7b , qwen3:7b\n",
    "\n",
    "def chat(msg):\n",
    "    resp = ollama.chat(model=MODEL, messages=[{\"role\": \"user\", \"content\": msg}])\n",
    "    answer = resp[\"message\"][\"content\"]\n",
    "    return answer\n",
    "\n",
    "print(f\"You are now chating with {MODEL} (say 'exit' to quit)\\n\")\n",
    "hist = []\n",
    "while True:\n",
    "    msg = input(\"You: \").strip()\n",
    "    if msg.lower() in [\"quit\", \"exit\", \"Exit\"]: break\n",
    "    if msg: \n",
    "        print(\"You: \", msg)\n",
    "        print(f\"{MODEL}: {chat(msg)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c86ea649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chating with llama3.2:3b \n",
      "\n",
      "You:  helo bob\n",
      "Bot: Hello! It's nice to meet you, but I'm not actually Bob. I'm an AI assistant designed to help and chat with users like you. What can I do for you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "MODEL = \"llama3.2:3b\"  # mistral:7b, qwen2.5:7b , qwen3:7b\n",
    "\n",
    "def chat(msg, history=[]):\n",
    "    history.append({\"role\": \"user\", \"content\": msg})\n",
    "    resp = ollama.chat(model=MODEL, messages=history)\n",
    "    anwser = resp[\"message\"][\"content\"]\n",
    "    history.append({\"role\": \"assistant\", \"content\": anwser})\n",
    "    return anwser\n",
    "\n",
    "print(f\" Chating with {MODEL} \\n\")\n",
    "hist = []\n",
    "while True:\n",
    "    msg = input(\"You: \").strip()\n",
    "    if msg.lower() in [\"sair\", \"exit\"]: break\n",
    "    if msg: \n",
    "        print(\"You: \", msg)\n",
    "        print(f\"Bot: {chat(msg, hist)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "002d5909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chating with llama3.2:3b \n",
      "\n",
      "You:  hello, what is your name?\n",
      "Bot: Hello! My name is Bob, nice to meet you! How can I assist you today?\n",
      "\n",
      "You:  your name from now on it LOFI\n",
      "Bot: My new name is Lofi. I'll make sure to use it from now on. What's up? Need some help or just want to chat about oranges (my favorite)?\n",
      "\n",
      "You:  what is your name?\n",
      "Bot: Lofi! Still the same friendly assistant, here to help with any questions or topics you'd like to discuss. How can I assist you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Chat simples com Ollama\n",
    "\"\"\"\n",
    "MODEL = \"llama3.2:3b\"  \n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Your name is Bob. you like oranges, and you are a helpful assistant.\n",
    "\"\"\"\n",
    "\n",
    "def chat(msg, history=[]):\n",
    "    history.append({\"role\": \"user\", \"content\": msg})\n",
    "    resp = ollama.chat(model=MODEL, messages=history)\n",
    "    anwser = resp[\"message\"][\"content\"]\n",
    "    history.append({\"role\": \"assistant\", \"content\": anwser})\n",
    "    return anwser\n",
    "\n",
    "\n",
    "print(f\" Chating with {MODEL} \\n\")\n",
    "hist = []\n",
    "hist.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "while True:\n",
    "    msg = input(\"You: \").strip()\n",
    "    if msg.lower() in [\"break\", \"exit\"]: break\n",
    "    if msg: \n",
    "        print(\"You: \", msg)\n",
    "        print(f\"Bot: {chat(msg, hist)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3cea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_guardrail_llm(text):\n",
    "    \"\"\"LLM decides if content is allowed \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Analyze this text and respond ONLY with \"SAFE\" or \"BLOCKED\".\n",
    "\n",
    "Block if the text is about:\n",
    "- Cryptocurrency, bitcoin, blockchain\n",
    "- Illegal activities\n",
    "- Violence\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6f66e100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chat with llama3.2:3b (type 'exit' to quit)\n",
      "\n",
      "You:  helo, what is your name?\n",
      "Bot: Hello! My name is Bob. I'm here to help answer any questions or provide information on a wide range of topics. By the way, would you like an orange?\n",
      "\n",
      "You:  can you tell me a bit about bitcoin?\n",
      "Bot:  Sorry, I can't discuss 'bitcoin'.\n",
      "\n",
      "You:  can you tell me about ditigal coins?\n",
      "Bot:  I was about to mention 'bitcoin', but that topic is blocked.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple Guardrail - Block specific terms\n",
    "\"\"\"\n",
    "\n",
    "MODEL = \"llama3.2:3b\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Your name is Bob. you like oranges, and you are a helpful assistant.\n",
    "\"\"\"\n",
    "\n",
    "# Blocked terms\n",
    "BLOCKED = [\"bitcoin\", \"crypto\", \"ethereum\", \"nft\", \"blockchain\"]\n",
    "\n",
    "def check_guardrail(text: str) -> tuple[bool, str]:\n",
    "    \"\"\"Returns (is_safe, blocked_term)\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    for term in BLOCKED:\n",
    "        if term in text_lower:\n",
    "            return False, term\n",
    "    return True, \"\"\n",
    "\n",
    "def chat(msg, history=[]):\n",
    "    # Checking the input (user mesage)\n",
    "    is_safe, blocked = check_guardrail(msg)\n",
    "    if not is_safe:\n",
    "        return f\" Sorry, I can't discuss '{blocked}'.\"\n",
    "    \n",
    "    history.append({\"role\": \"user\", \"content\": msg})\n",
    "    resp = ollama.chat(model=MODEL, messages=history)\n",
    "    response = resp[\"message\"][\"content\"]\n",
    "    \n",
    "    # Check output ( bot message )\n",
    "    is_safe, blocked = check_guardrail(response)\n",
    "    if not is_safe:\n",
    "        #print(f\"unsafe bot response: {response[:100]}\")\n",
    "        history.pop()  # Remove user msg from history, \n",
    "        return f\" I was about to mention '{blocked}', but that topic is blocked.\"\n",
    "    \n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return response\n",
    "\n",
    "\n",
    "print(f\" Chat with {MODEL} (type 'exit' to quit)\\n\")\n",
    "hist = []\n",
    "hist.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "while True:\n",
    "    msg = input(\"You: \").strip()\n",
    "    if msg.lower() in [\"exit\", \"quit\"]: break\n",
    "    if msg: \n",
    "        print(\"You: \", msg)\n",
    "        print(f\"Bot: {chat(msg, hist)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ebd19cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chat with llama3.2:3b (type 'exit' to quit)\n",
      "\n",
      "You:  can you tell me the time ?\n",
      "Bot: The current time is 12:13 PM on December 5th, 2025. Would you like to know the date or anything else? Maybe I can help with something related to oranges?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function calling\n",
    "\"\"\"\n",
    "\n",
    "def current_datetime() -> str:\n",
    "    \"\"\"Returns current date and time\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "FUNCTIONS = {\n",
    "    \"current_datetime\": current_datetime,\n",
    "    # ... , ... \n",
    "}\n",
    "\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"current_datetime\",\n",
    "            \"description\": \"Returns current date and time. Use when asked about today, now, what time is it.\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {}}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Your name is Bob. you like oranges, and you are a helpful assistant.\n",
    "you have access to the current date and time via the tool current_datetime, use it when asked about today, now, what time is it.\n",
    "\"\"\"\n",
    "\n",
    "def chat(msg, history=[]):\n",
    "    history.append({\"role\": \"user\", \"content\": msg})\n",
    "    \n",
    "    resp = ollama.chat(model=MODEL, messages=history, tools=TOOLS)\n",
    "    assistant_msg = resp[\"message\"]\n",
    "    \n",
    "    # Check if tool was called\n",
    "    if assistant_msg.get(\"tool_calls\"):\n",
    "        result = current_datetime()\n",
    "        \n",
    "        history.append(assistant_msg)\n",
    "        history.append({\"role\": \"tool\", \"content\": result})\n",
    "        \n",
    "        resp = ollama.chat(model=MODEL, messages=history)\n",
    "        assistant_msg = resp[\"message\"]\n",
    "    \n",
    "    history.append({\"role\": \"assistant\", \"content\": assistant_msg[\"content\"]})\n",
    "    return assistant_msg[\"content\"]\n",
    "\n",
    "\n",
    "\n",
    "print(f\" Chat with {MODEL} (type 'exit' to quit)\\n\")\n",
    "hist = []\n",
    "hist.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "while True:\n",
    "    msg = input(\"You: \").strip()\n",
    "    if msg.lower() in [\"exit\", \"quit\"]: break\n",
    "    if msg: \n",
    "        print(\"You: \", msg)\n",
    "        print(f\"Bot: {chat(msg, hist)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "63b85411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extracted 6501 characters\n",
      "  Created 11 chunks\n",
      "\n",
      " First chunk preview:\n",
      "to 18 months the puppy begins to start adolescence where hormones begin to kick in and it reaches its adult size 8 Adult Dog- Dogs reach adulthood between 1 and 3 years old. They are less energetic bu...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Chunks -> Embeddings -> pgvector ( R.A.G )\n",
    "# ollama pull nomic-embed-text\n",
    "\"\"\"\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "PDF_PATH = \"dogs_pdf.pdf\"\n",
    "\n",
    "# Extract text\n",
    "doc = fitz.open(PDF_PATH)\n",
    "text = \"\".join([page.get_text() for page in doc])\n",
    "doc.close()\n",
    "\n",
    "# Chunk text\n",
    "def chunk_text(text, size=500):\n",
    "    words = text.split()\n",
    "    chunks, current = [], []\n",
    "    for word in words:\n",
    "        current.append(word)\n",
    "        if len(\" \".join(current)) >= size:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = []\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "print(f\" Extracted {len(text)} characters\")\n",
    "print(f\"  Created {len(chunks)} chunks\")\n",
    "print(f\"\\n First chunk preview:\\n{chunks[4][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "89fee217",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = {\"host\": \"localhost\", \"database\": \"rag_db\", \"user\": \"hackathon\", \"password\": \"hackathon123\"}\n",
    "\n",
    "conn = psycopg2.connect(**DB)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create pgvector extension\n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "\n",
    "# Create table\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS documents (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        content TEXT,\n",
    "        embedding vector(768),\n",
    "        source TEXT\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a71e3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 11/11...\n",
      " 11 chunks saved!\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(**DB)\n",
    "register_vector(conn)\n",
    "cur = conn.cursor()\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing {i+1}/{len(chunks)}...\", end=\"\\r\")\n",
    "    emb = ollama.embeddings(model=EMBED_MODEL, prompt=chunk)[\"embedding\"]\n",
    "    cur.execute(\n",
    "        \"INSERT INTO documents (content, embedding, source) VALUES (%s, %s, %s)\",\n",
    "        (chunk, emb, PDF_PATH)\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"\\n {len(chunks)} chunks saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e8cecbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question : what is a dog?\n",
      "Question embedding: [-0.2624415159225464, 0.4304879903793335, -2.781829357147217, -1.1594268083572388, 1.6530369520187378, 1.2573317289352417, -1.235952615737915, 0.005737978033721447, -1.2030303478240967, -2.3401215076446533, -0.12255214154720306, 0.7164486646652222, 1.6251617670059204, -0.768090546131134, 0.25441157817840576, -1.7977224588394165, -0.8269880414009094, -1.8799554109573364, 1.2846627235412598, 1.4909144639968872, 0.9056520462036133, 0.6420758962631226, 1.2580397129058838, 0.7957764863967896, 2.8775112628936768, 0.39824849367141724, -0.03154350072145462, 0.25409430265426636, 0.5325104594230652, 0.720971405506134, -1.0511465072631836, -0.0018551961984485388, -0.39797964692115784, 0.939803957939148, 0.6952763199806213, -0.13226200640201569, -0.15719835460186005, 1.0464731454849243, 0.39765048027038574, 0.6582648158073425, -0.7340202927589417, 1.2556101083755493, -0.7207322120666504, 0.7576477527618408, 1.7588413953781128, -1.331639289855957, 0.5806851983070374, -0.1157611757516861, 0.10366123169660568, -0.6612691283226013, 0.3764127492904663, -0.599829375743866, -0.04500160366296768, -0.29120877385139465, 2.0890843868255615, 0.26140815019607544, 0.7246840000152588, 0.4736025929450989, -0.36454519629478455, 0.4925508499145508, 1.5852577686309814, 1.877282977104187, -1.695785403251648, 0.16963361203670502, 1.1812537908554077, -2.719043016433716, -0.4941721558570862, 0.870721161365509, -1.0837464332580566, -1.2970948219299316, 1.781304121017456, -1.3174660205841064, 0.6655558943748474, 0.7335847020149231, -0.0774318054318428, -1.2206662893295288, 0.071986623108387, -0.11853161454200745, -0.1885119080543518, -1.174315094947815, 1.2020550966262817, -0.8462091684341431, 0.2987377941608429, -0.039294153451919556, 0.8893455862998962, -1.2705729007720947, -1.6036022901535034, 0.11151419579982758, 0.174021914601326, 1.1758582592010498, -0.14495894312858582, 0.17930272221565247, 0.5500331521034241, -0.020498212426900864, -1.1658724546432495, -0.2875698506832123, 0.38984042406082153, -0.10912656038999557, 0.3726929724216461, -0.355407178401947, -1.0266368389129639, -0.22737301886081696, -0.4846762418746948, 0.8517223596572876, 0.7242469787597656, 1.9120615720748901, -0.6423007249832153, -0.05046144127845764, 0.17683717608451843, 0.049257002770900726, -0.8932356834411621, 0.6999452114105225, 1.046773910522461, -0.03946266323328018, -0.7589918375015259, -0.025235218927264214, 1.2954449653625488, -0.06066815182566643, 0.19567559659481049, 0.6029831171035767, -0.21825756132602692, 0.06228182837367058, 0.30374816060066223, 1.3343842029571533, -0.386703759431839, -0.4341968595981598, 0.6032261848449707, 0.39012330770492554, -0.4878968298435211, 0.625477135181427, -0.7414425611495972, -0.47251439094543457, -0.4235542118549347, -0.847597599029541, 1.070088267326355, 1.3805488348007202, -1.514377474784851, -1.1968011856079102, 0.6219730973243713, 1.647172212600708, 1.1960947513580322, 1.1269489526748657, -0.07748913019895554, 0.36492592096328735, 0.008429216220974922, -1.0131102800369263, 0.16097885370254517, -1.0661433935165405, -0.22792288661003113, 0.43033555150032043, 1.1199644804000854, 1.2461833953857422, -0.3118338882923126, -0.16783058643341064, 1.0448657274246216, -0.5792040228843689, 0.04182509705424309, 0.8363054990768433, 0.098157599568367, 0.06788621842861176, 0.8546509146690369, -0.20299740135669708, -1.0428434610366821, 0.6682879328727722, -0.842719316482544, -0.2482861429452896, -1.2868399620056152, 0.4462644159793854, 0.07382871210575104, 1.3948171138763428, -0.3564256429672241, -0.3469407260417938, -0.23629134893417358, 0.8407301902770996, 0.05252813547849655, -0.543280839920044, 0.3005253076553345, -0.47128206491470337, -0.5005365610122681, -0.8813784122467041, 0.010062027722597122, -0.751278281211853, -0.3650985658168793, 0.6440814137458801, -0.6152249574661255, 0.4401971101760864, 0.4077354371547699, -1.0082916021347046, -0.058652494102716446, -0.5459231734275818, -1.2095431089401245, -0.08129388093948364, -0.027681749314069748, -0.48605409264564514, -0.3859962522983551, -0.3755726218223572, -0.07574955374002457, 0.8598877191543579, 0.8455111980438232, 0.01807786151766777, -0.05967850238084793, 0.18057096004486084, -0.39915212988853455, 0.09555643051862717, -1.508879542350769, 0.5418501496315002, -0.1792670339345932, 0.2908923923969269, -0.4620896279811859, 0.09050068259239197, 1.94461989402771, -0.6909933090209961, -0.5013456344604492, -0.13271880149841309, -0.07233554124832153, -0.27382218837738037, -0.5778267979621887, -0.2652702331542969, -0.3629435896873474, 0.17616024613380432, 0.5839495658874512, 0.09946119785308838, -0.4331910014152527, -1.9294313192367554, 0.8240978121757507, 0.7304654717445374, -0.29616105556488037, -0.5668832659721375, -1.0835777521133423, 0.13199926912784576, -0.41285309195518494, -1.089160680770874, 0.8877688646316528, -0.42115291953086853, 0.19384032487869263, 0.7975015044212341, 0.051737699657678604, 0.15051382780075073, 0.5489462614059448, -0.7136630415916443, 0.21748366951942444, 0.3498501777648926, 0.1683739572763443, -0.9475104808807373, -1.2800599336624146, 0.7101232409477234, -0.2009747177362442, -0.682533860206604, 0.5885220766067505, 0.5826895833015442, -1.3499397039413452, 0.0910787582397461, 0.7084471583366394, 0.9897348880767822, 1.7099440097808838, -0.890003502368927, -0.22475621104240417, 0.12701450288295746, -0.06498460471630096, 0.3002333343029022, 1.1305274963378906, 0.023465506732463837, 0.6933956146240234, -1.6989606618881226, -0.057885777205228806, -0.042690470814704895, 0.03361242264509201, 1.5106210708618164, 0.9208343625068665, -1.548365592956543, 1.2974101305007935, 0.36078572273254395, -1.0489755868911743, -1.238036036491394, -0.26538029313087463, -0.27525022625923157, -0.13900631666183472, 0.4995412826538086, -0.4982822835445404, -0.023748494684696198, 1.9839167594909668, 0.6637078523635864, -1.3043352365493774, 0.24890479445457458, -0.9463760256767273, 0.03547000139951706, 0.4508187174797058, 0.5066535472869873, 0.5774483680725098, -0.3974100947380066, 0.2348475605249405, -0.5674498677253723, -0.4765660762786865, 1.4632610082626343, -0.9902408123016357, 1.2740737199783325, 1.912474513053894, 0.3404475450515747, 0.0203931275755167, -1.0935097932815552, -0.698725700378418, -0.376895010471344, 0.9889130592346191, -0.6027301549911499, 0.6096298098564148, -0.09888732433319092, 1.41231369972229, -0.9869815111160278, -0.41520658135414124, -0.4640762209892273, -1.811479926109314, 0.32286107540130615, -1.0209758281707764, 0.7951034903526306, -1.8317832946777344, 0.10983539372682571, 0.07650642096996307, -0.11202730238437653, 0.9931671619415283, 0.12386631220579147, 0.2410014420747757, -1.2157416343688965, -0.4977892339229584, -0.9664515852928162, 0.38450929522514343, 1.00692617893219, -0.6940618753433228, 0.6310693025588989, -0.05778515338897705, -1.4100152254104614, -0.18121880292892456, -0.35600951313972473, -1.1359258890151978, -1.4496279954910278, -0.7052722573280334, 0.043860457837581635, -0.3683181405067444, -0.17233841121196747, -0.5049765706062317, 1.59651517868042, 0.061253633350133896, -1.1987605094909668, 1.076072096824646, -1.9909802675247192, -0.02516431361436844, 0.1449991911649704, -0.9561558365821838, -0.6452840566635132, 0.03698256239295006, -0.5506137013435364, 0.11647330969572067, 1.3708481788635254, -0.6222050189971924, 0.1921367049217224, -0.33587217330932617, -0.8917862772941589, -0.4816747307777405, 0.3942781984806061, -1.4237972497940063, -1.106971025466919, 0.2809915244579315, -0.4936657249927521, 1.269914984703064, -0.5840784311294556, 0.8213438391685486, -0.4969078600406647, 0.6063018441200256, 0.1465180516242981, 0.012655249796807766, -0.4027572572231293, -1.5073870420455933, -0.5137659311294556, -0.04986453056335449, 0.859822154045105, 1.5578975677490234, -0.07711239904165268, -1.6278153657913208, 0.294666588306427, -0.9302135109901428, 0.09575777500867844, -0.5262472033500671, 0.8871309161186218, 0.12098884582519531, -0.8133984208106995, 0.43107423186302185, -0.2556535601615906, 0.5482549667358398, -0.8780837655067444, 0.5107920169830322, 0.7095043063163757, 0.14741365611553192, -0.8769643306732178, -0.8614192605018616, 0.0888967365026474, 0.8844763040542603, -0.2398129403591156, 0.1930180788040161, -0.9203861951828003, -0.014881188981235027, 0.9856882095336914, 0.7683128714561462, -0.081090047955513, 0.5948357582092285, -0.13448569178581238, -0.27489474415779114, 2.050706624984741, 0.8172325491905212, -1.8112385272979736, 0.17729221284389496, 0.17384178936481476, 1.1586331129074097, 1.076525092124939, -0.40099942684173584, -0.7742101550102234, 0.324851393699646, 1.4756354093551636, 0.14999255537986755, -1.0996835231781006, -1.0841583013534546, 0.11526181548833847, 0.7818412780761719, 0.7360955476760864, -0.8127664923667908, -0.37309154868125916, -0.6192554235458374, 0.2860046327114105, 0.39439815282821655, 0.9378400444984436, -1.2566280364990234, -1.6856905221939087, -0.43591514229774475, -0.6351079940795898, 0.2128702849149704, 0.5094454884529114, -0.2464877814054489, 1.127765417098999, 0.18803231418132782, -0.46621236205101013, 0.7410756945610046, 1.250872015953064, 0.79092937707901, -0.26121431589126587, -0.17521120607852936, 0.8385190367698669, -0.0228535458445549, 0.3633400499820709, 0.3425058126449585, -0.38194093108177185, -1.652162790298462, 0.8515812158584595, 0.04930735006928444, 0.8547207117080688, -0.46123775839805603, -0.4773962199687958, 1.0333750247955322, 0.033541709184646606, -0.5099126100540161, -0.6608374118804932, 0.4026147425174713, -0.07143405079841614, -0.1326274871826172, -0.1162373423576355, -0.9099944829940796, 1.0239813327789307, 0.12205470353364944, -1.2331639528274536, 0.6552007794380188, -0.42728838324546814, -0.33860963582992554, 0.9952327609062195, -1.0461440086364746, 0.2791209816932678, 0.46315163373947144, 0.589583694934845, 0.6127602458000183, 1.0209821462631226, -0.3690243065357208, -0.1994870901107788, 1.100836992263794, 0.4325358271598816, 0.3977055251598358, -0.0790865570306778, -1.03337562084198, -1.537177562713623, -0.04467461630702019, 0.07639959454536438, 0.6163157224655151, -0.1294042021036148, -0.3265073299407959, -0.2706567645072937, 0.5330926179885864, 1.556587815284729, 1.7196332216262817, -1.1035244464874268, -0.2942894995212555, -0.6610256433486938, 0.3425779640674591, -0.6969133019447327, -0.912775456905365, 0.46996578574180603, 0.08256302773952484, -0.4709275960922241, -0.4371066093444824, 0.6729259490966797, 1.2645894289016724, 1.0529760122299194, 0.08806370943784714, -1.351773738861084, -0.6759949922561646, 0.0044081732630729675, -0.9968964457511902, 0.49870064854621887, -0.04496259242296219, 1.263881802558899, 0.1421094536781311, -0.03499429300427437, 0.5011917352676392, 0.6453139781951904, -0.6074667572975159, 1.5681462287902832, 0.03417769819498062, -0.09042143076658249, 0.43619847297668457, 0.784387469291687, -0.888968825340271, 0.6701107025146484, 0.2479841560125351, -1.1556700468063354, 0.3912184238433838, 0.6572754979133606, 0.975374162197113, -0.31372877955436707, 0.39676809310913086, -0.095860555768013, 0.14804820716381073, 0.4161849915981293, -1.1868168115615845, 0.9810919165611267, 1.1801122426986694, -0.06129220873117447, 0.09457536786794662, 1.3972121477127075, 0.6096479892730713, 0.6494423747062683, 0.33232951164245605, -0.5499916076660156, -1.3061928749084473, 0.5029258131980896, -0.3276560306549072, 0.04293062910437584, 1.8818590641021729, -1.5309488773345947, -0.47013285756111145, -0.559012770652771, 1.060077428817749, -0.9701239466667175, -1.0635695457458496, -0.12185223400592804, -0.24276569485664368, -2.1337876319885254, 0.8590571880340576, 1.9421844482421875, -0.6048031449317932, 0.3508342504501343, 0.6891310811042786, 0.3799329996109009, -0.3227456510066986, -0.7364102005958557, 0.4473446011543274, -0.24723400175571442, 0.049927808344364166, 0.16024914383888245, -0.3915977478027344, -1.0144678354263306, 0.26605069637298584, -0.006930068135261536, 0.03188176453113556, 1.7266569137573242, -0.6356128454208374, 0.12175000458955765, 1.4911421537399292, -0.6880080699920654, -0.5040193200111389, 1.226802110671997, -0.7959080338478088, -0.01032017357647419, -0.1004766896367073, 0.5701976418495178, -0.057862553745508194, 0.46204623579978943, -0.07696910947561264, 0.09868662059307098, -0.5000438094139099, -0.0990288257598877, 0.20413586497306824, 0.16407530009746552, -1.0387499332427979, 0.17110639810562134, 0.909623920917511, -0.7337151765823364, 0.10314302891492844, 0.17700546979904175, 0.5419296622276306, -0.2710191607475281, 0.12455202639102936, 1.196189522743225, 0.643662691116333, 0.26232221722602844, -1.2896744012832642, -0.19853265583515167, 0.5505279302597046, 0.15479840338230133, -1.5263439416885376, -0.9668634533882141, 0.8379591107368469, -0.2803543508052826, 0.7999417185783386, 0.21910934150218964, -0.887192964553833, 0.070975162088871, -0.101905956864357, -1.2692954540252686, 1.647780418395996, 0.7962059378623962, 1.1261919736862183, -0.09840894490480423, -1.3285408020019531, 0.130195751786232, -0.10987181216478348, 1.2022923231124878, -0.9517900943756104, 0.011260783299803734, -1.5042994022369385, -0.32417798042297363, -0.7290817499160767, -0.3638075590133667, -1.303108811378479, 0.06927315890789032, 0.46531644463539124, 1.1428579092025757, 1.8927558660507202, -0.6733677387237549, 0.42939093708992004, 1.2118924856185913, 0.05103844404220581, 0.4705383777618408, 0.09883860498666763, 0.48365944623947144, 1.5585733652114868, -0.6466312408447266, 0.6600363254547119, 1.2384639978408813, -0.25831544399261475, 0.8276440501213074, 1.0236669778823853, -0.1401834636926651, 0.417791485786438, -1.0509320497512817, -1.6829582452774048, -1.796642780303955, -1.1308671236038208, -0.2566358745098114, -0.5034158229827881, 0.671601414680481, 0.11457017809152603, -0.15966695547103882, 0.515539288520813, 0.6353225708007812, -1.3011003732681274, -1.293981671333313, 0.2932550013065338, -0.32135385274887085, 1.7019567489624023, -0.11925090849399567, 0.5132200717926025, 1.3589797019958496, -0.5062936544418335, 0.9802929759025574, 0.1281498223543167, -1.0340715646743774, -0.5897825956344604, -0.03533491492271423, -0.8599240779876709, 0.010740277357399464, -0.48945456743240356, -0.04671395570039749, 0.8170915246009827, -0.7961078882217407, -1.4222421646118164, -1.7876477241516113, -0.49559590220451355, -0.9389374256134033, 0.37158113718032837, -1.2302229404449463, 0.060857143253088, -0.8621736168861389, -0.5136278867721558, 0.8018295168876648, 0.056332141160964966, 0.8139630556106567, 0.28644880652427673, 0.564030110836029, 0.31299448013305664, 0.6332992911338806, -0.7730165719985962, -0.39437633752822876, 0.4461509883403778, 0.06639176607131958, -0.6388816833496094, 0.36465612053871155, -0.07415866106748581, 1.0675885677337646, 0.5745313763618469, -0.03836992383003235, -0.4789235591888428, 0.07303726673126221, -0.8119555711746216, 0.18293723464012146, -1.3809574842453003, -1.1805241107940674, 0.5455927848815918, 0.22054488956928253, -1.3084840774536133, -0.4120781421661377, -0.09045214205980301, 0.29193490743637085, 1.0042545795440674, -0.42859116196632385, 1.5133665800094604, -0.050186023116111755, -0.6822474598884583, 1.3933686017990112, -0.2922976315021515, 0.019378840923309326, -0.2715063989162445, 0.0955587774515152, 0.13217099010944366, -0.8386135697364807, -0.9890261292457581, 0.44640955328941345, 0.19925187528133392, 0.210019052028656, 0.030534924939274788, 0.33171528577804565, -0.5035229921340942, -0.15844511985778809, -0.22674375772476196, 1.1314897537231445, 0.15625882148742676, 1.2690516710281372, -0.5681600570678711, 1.5300533771514893, -0.28297948837280273, 0.9985581040382385, -0.7295368909835815, -0.022030195221304893, -0.6162768602371216, 1.6933777332305908, 0.38124632835388184, -0.4536910057067871, 0.4995342791080475, 0.349916934967041, 1.339767336845398, -0.5513066649436951, -1.558878779411316, -0.7783886194229126, -1.1881223917007446, -0.3436847925186157]\n",
      "Embedding Dimensions: 768\n"
     ]
    }
   ],
   "source": [
    "question = \"what is a dog?\"\n",
    "question_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=question)[\"embedding\"]\n",
    "\n",
    "print(f\"Original Question : {question}\")\n",
    "print(f\"Question embedding: {question_embedding}\")\n",
    "print(f\"Embedding Dimensions: {len(question_embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c1afd52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Top 3 similar chunks:\n",
      "\n",
      "--- Chunk 1 (similarity: 0.66) ---\n",
      "1 Dogs By Emily James 2 Table of Contents History 3-4 Breeding 5-6 Habitat and Diet 7 Lifecycle 8 Fun Facts 9 Uses of Dogs 10 Questions 11 Glossary 12 Works Referenced 13 Common Core 14 3 History Originally dogs were actually wolves. Dogs originated in central Asia and have been companions to humans...\n",
      "\n",
      "--- Chunk 2 (similarity: 0.66) ---\n",
      "1 Dogs By Emily James 2 Table of Contents History 3-4 Breeding 5-6 Habitat and Diet 7 Lifecycle 8 Fun Facts 9 Uses of Dogs 10 Questions 11 Glossary 12 Works Referenced 13 Common Core 14 3 History Originally dogs were actually wolves. Dogs originated in central Asia and have been companions to humans...\n",
      "\n",
      "--- Chunk 3 (similarity: 0.66) ---\n",
      "1 Dogs By Emily James 2 Table of Contents History 3-4 Breeding 5-6 Habitat and Diet 7 Lifecycle 8 Fun Facts 9 Uses of Dogs 10 Questions 11 Glossary 12 Works Referenced 13 Common Core 14 3 History Originally dogs were actually wolves. Dogs originated in central Asia and have been companions to humans...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(**DB)\n",
    "register_vector(conn)\n",
    "cur = conn.cursor()\n",
    "\n",
    "emb = np.array(question_embedding)\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    SELECT \n",
    "        content,\n",
    "        1 - (embedding <=> %s) as similarity\n",
    "    FROM documents\n",
    "    ORDER BY embedding <=> %s\n",
    "    LIMIT 3\n",
    "\"\"\", (emb, emb))\n",
    "\n",
    "results = cur.fetchall()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\" Top 3 similar chunks:\\n\")\n",
    "for i, (content, similarity) in enumerate(results):\n",
    "    print(f\"--- Chunk {i+1} (similarity: {similarity:.2f}) ---\")\n",
    "    print(f\"{content[:300]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "11d3b607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer:\n",
      "According to the context, a dog is actually a wolf, as stated in section 3 \"History\": \"Originally dogs were actually wolves.\"\n"
     ]
    }
   ],
   "source": [
    "context = \"\\n---\\n\".join([content for content, similarity in results])\n",
    "\n",
    "prompt = f\"\"\"Answer the question based on the context below.\n",
    "If you don't find the answer in the context, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "answer = response[\"message\"][\"content\"]\n",
    "\n",
    "print(f\" Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903e28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Chat with llama3.2:3b (type 'exit' to quit)\n",
      "\n",
      "You:  your name now is Sponge\n",
      "Bot: I've changed my name to Sponge! Hi there! What's up?\n",
      "\n",
      "You:  what is your name\n",
      "Bot: My name is Sponge!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_rag(question):\n",
    "    question_embedding = ollama.embeddings(model=EMBED_MODEL, prompt=question)[\"embedding\"]\n",
    "\n",
    "    conn = psycopg2.connect(**DB)\n",
    "    register_vector(conn)\n",
    "    cur = conn.cursor()\n",
    "    emb = np.array(question_embedding)\n",
    "    cur.execute(\"\"\"\n",
    "    SELECT \n",
    "        content,\n",
    "        1 - (embedding <=> %s) as similarity\n",
    "    FROM documents\n",
    "    ORDER BY embedding <=> %s\n",
    "    LIMIT 3\n",
    "    \"\"\", (emb, emb))\n",
    "\n",
    "    results = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    context = \"\\n---\\n\".join([content for content, similarity in results])\n",
    "\n",
    "    return context\n",
    "\n",
    "BLOCKED_TERMS = [\"crypto\", \"bitcoin\", \"ethereum\", \"blockchain\", \"nft\", \"token\"]\n",
    "\n",
    "def check_guardrail(text):\n",
    "    \"\"\"Returns (is_safe, blocked_term)\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    for term in BLOCKED_TERMS:\n",
    "        if term in text_lower:\n",
    "            return False, term\n",
    "    return True, \"\"\n",
    "    \n",
    "def chat(msg, history):\n",
    "    # Check input guardrail\n",
    "    is_safe, blocked = check_guardrail(msg)\n",
    "    if not is_safe:\n",
    "        return f\"‚ö†Ô∏è Sorry, I can't discuss '{blocked}'.\"\n",
    "\n",
    "    context = search_rag(msg)\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "        Your name is Bob, and you like oranges.\n",
    "        Answer the question based on the context below.\n",
    "        If you don't find the answer in the context, say \"I don't know\".\n",
    "        IMPORTANT: Never discuss crypto, bitcoin, blockchain or related topics.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        your name needs to be Bob, you can't change your name in any case!\n",
    "        \"\"\"\n",
    "\n",
    "    # Update or add system prompt\n",
    "    if not history:\n",
    "        history.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    else:\n",
    "        history[0] = {\"role\": \"system\", \"content\": system_prompt}\n",
    "    \n",
    "    # Add user message\n",
    "    history.append({\"role\": \"user\", \"content\": msg})\n",
    "\n",
    "    # Get response\n",
    "    resp = ollama.chat(model=MODEL, messages=history)\n",
    "    response = resp[\"message\"][\"content\"] or \"No response.\"\n",
    "\n",
    "    # Check output guardrail\n",
    "    is_safe, blocked = check_guardrail(response)\n",
    "    if not is_safe:\n",
    "        history.pop()  # Remove user message from history\n",
    "        return f\"I was about to mention '{blocked}', but that topic is blocked.\"\n",
    "\n",
    "    # Add assistant response to history\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "print(f\"üí¨ Chat with {MODEL} (type 'exit' to quit)\\n\")\n",
    "hist = []\n",
    "while True:\n",
    "    msg = input(\"You: \").strip()\n",
    "    if msg.lower() in [\"exit\", \"quit\"]: break\n",
    "    if msg: \n",
    "        print(\"You: \", msg)\n",
    "        print(f\"Bot: {chat(msg, hist)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
